{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "cf6d5d8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "from numpy.linalg import norm\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader, TensorDataset, Subset, random_split\n",
    "from inference_attack import *\n",
    "from model import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "691f1c3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# config\n",
    "dataset_name = \"MNIST\"         # \"MNIST\" or \"CIFAR10\"\n",
    "data_path = \"./data\"\n",
    "batch_size = 8                   # Must be 1 for per-sample gradient\n",
    "learning_rate = 0.001\n",
    "num_classes = 10\n",
    "use_pretrained_model = False\n",
    "inference_dataset_size = 100\n",
    "global_model_path = \"trained_model_MNIST.pth\"\n",
    "target_model_path = \"global_update.pth\"\n",
    "criterion_global_model = torch.nn.CrossEntropyLoss()\n",
    "criterion_grad_classifier = torch.nn.BCELoss()\n",
    "\n",
    "train_size = 0.7\n",
    "test_size = 1 - train_size\n",
    "num_epochs = 15\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "a09708db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset (we use test datasets to train the model on samples that have not been used in training of the global model)\n",
    "if dataset_name == \"MNIST\":\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.1307,), (0.3081,))\n",
    "    ])\n",
    "    dataset = datasets.MNIST(root=data_path, train=False, download=True, transform=transform)\n",
    "    input_shape = (1, 28, 28)\n",
    "    #input_channels = 1\n",
    "    #flatten_size = 64 * 6 * 6\n",
    "\n",
    "elif dataset_name == \"CIFAR10\":\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.4914, 0.4822, 0.4465),\n",
    "                             (0.2470, 0.2435, 0.2616))\n",
    "    ])\n",
    "    dataset = datasets.CIFAR10(root=data_path, train=False, download=True, transform=transform)\n",
    "    input_shape = (3, 32, 32)\n",
    "    # input_channels = 3\n",
    "    # flatten_size = 64 * 6 * 6\n",
    "\n",
    "else:\n",
    "    raise ValueError(f\"Unsupported dataset: {dataset_name}\")\n",
    "\n",
    "indices = random.sample(range(len(dataset)), inference_dataset_size)\n",
    "dataset = Subset(dataset, indices=indices)\n",
    "\n",
    "dataloader = DataLoader(dataset, batch_size=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "88bc0655",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load global model and target update\n",
    "theta_0 = torch.load(global_model_path)\n",
    "theta_1 = torch.load(target_model_path)\n",
    "\n",
    "inference_model = Model1(input_shape, num_classes).to(device)\n",
    "inference_model.load_state_dict(state_dict=theta_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "b4b792e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "124682 10\n"
     ]
    }
   ],
   "source": [
    "# collect gradient features\n",
    "features = []\n",
    "labels = []\n",
    "\n",
    "inference_model.to(device)\n",
    "inference_model.eval() # TODO: train or eval???\n",
    "\n",
    "for x, y in dataloader:\n",
    "    x, y = x.to(device), y.to(device)\n",
    "\n",
    "    # Forward pass\n",
    "    output = inference_model(x)\n",
    "    loss = criterion_global_model(output, y)\n",
    "\n",
    "    # print(\"pr:\", output, \"gt:\", y)\n",
    "\n",
    "    # Compute gradients w.r.t. model parameters\n",
    "    grad = torch.autograd.grad(loss, inference_model.parameters(), retain_graph=False)\n",
    "    # grad = torch.autograd.grad(loss, [inference_model.fc1.weight, inference_model.fc1.bias], retain_graph=False)\n",
    "    grad_vector = torch.cat([g.view(-1) for g in grad])  # Flatten and concatenate\n",
    "\n",
    "    # Detach and store\n",
    "    features.append(grad_vector.detach().cpu().float())\n",
    "    labels.append(one_hot_encode(y.item(), num_classes))\n",
    "    # labels.append(y)\n",
    "\n",
    "features = torch.stack(features)\n",
    "labels = torch.stack(labels).float()\n",
    "#labels = torch.stack([y.detach().clone().float32() for y in labels])\n",
    "\n",
    "dataset = TensorDataset(features, labels)\n",
    "input_dim = features.shape[1]\n",
    "output_dim = labels.shape[1]\n",
    "\n",
    "print(input_dim, output_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "bda05714",
   "metadata": {},
   "outputs": [],
   "source": [
    "# estimate the target gradient update as theta_0 - theta_1 / lr\n",
    "#est_grad = elementwise_diff_state_dicts(theta_0, theta_1, learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "c369e2c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_grad_classifier(model, train_loader, test_loader, criterion, num_epochs=10, lr=1e-3, device='cuda'):\n",
    "    model = model.to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        for x_batch, y_batch in train_loader:\n",
    "            x_batch = x_batch.to(device)\n",
    "            y_batch = y_batch.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(x_batch)  # raw logits\n",
    "\n",
    "            loss = criterion(outputs, y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item() * x_batch.size(0)\n",
    "\n",
    "            # Accuracy\n",
    "            #preds = torch.argmax(outputs, dim=1)\n",
    "            #correct += (preds == y_batch).sum().item()\n",
    "            total += y_batch.size(0)\n",
    "\n",
    "        avg_loss = running_loss / total\n",
    "        accuracy = 0\n",
    "        #accuracy = correct / total * 100\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs} | Loss: {avg_loss:.4f} | Accuracy: {accuracy:.2f}%\")\n",
    "        evaluate_model(model, test_loader, criterion, device)\n",
    "        \n",
    "def evaluate_model(model, test_loader, criterion, device='cuda'):\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "\n",
    "    total_loss = 0.0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    cosine_sims = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for x_batch, y_batch in test_loader:\n",
    "            x_batch = x_batch.to(device)\n",
    "            y_batch = y_batch.to(device)\n",
    "\n",
    "            outputs = model(x_batch)  # sigmoid output\n",
    "            loss = criterion(outputs, y_batch)\n",
    "            total_loss += loss.item() * x_batch.size(0)\n",
    "\n",
    "            preds = outputs.cpu().numpy()\n",
    "            labels = y_batch.cpu().numpy()\n",
    "\n",
    "            all_preds.append(preds)\n",
    "            all_labels.append(labels)\n",
    "\n",
    "            # Compute cosine similarity per sample\n",
    "            for p, l in zip(preds, labels):\n",
    "                # Avoid division by zero\n",
    "                if norm(p) == 0 or norm(l) == 0:\n",
    "                    cosine_sims.append(0.0)\n",
    "                else:\n",
    "                    cos_sim = np.dot(p, l) / (norm(p) * norm(l))\n",
    "                    cosine_sims.append(cos_sim)\n",
    "\n",
    "    avg_loss = total_loss / len(test_loader.dataset)\n",
    "    all_preds = np.vstack(all_preds)\n",
    "    all_labels = np.vstack(all_labels)\n",
    "    avg_cosine_similarity = np.mean(cosine_sims)\n",
    "\n",
    "    print(f\"Test Loss: {avg_loss:.4f} | Cosine Similarity: {avg_cosine_similarity:.4f}\")\n",
    "\n",
    "    return all_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "f3cba6f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70 30\n"
     ]
    }
   ],
   "source": [
    "# dataset preparation\n",
    "tmp_train = int(train_size*inference_dataset_size)\n",
    "tmp_test = int(inference_dataset_size-train_size*inference_dataset_size)\n",
    "print(tmp_train, tmp_test)\n",
    "train_dataset, test_dataset = random_split(dataset, [tmp_train, tmp_test])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "9834dc7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15 | Loss: 0.6580 | Accuracy: 0.00%\n",
      "Test Loss: 0.5847 | Cosine Similarity: 0.3840\n",
      "Epoch 2/15 | Loss: 0.5914 | Accuracy: 0.00%\n",
      "Test Loss: 0.5547 | Cosine Similarity: 0.3844\n",
      "Epoch 3/15 | Loss: 0.4913 | Accuracy: 0.00%\n",
      "Test Loss: 0.5433 | Cosine Similarity: 0.3756\n",
      "Epoch 4/15 | Loss: 0.4322 | Accuracy: 0.00%\n",
      "Test Loss: 0.4846 | Cosine Similarity: 0.3973\n",
      "Epoch 5/15 | Loss: 0.4138 | Accuracy: 0.00%\n",
      "Test Loss: 0.4089 | Cosine Similarity: 0.4270\n",
      "Epoch 6/15 | Loss: 0.3246 | Accuracy: 0.00%\n",
      "Test Loss: 0.3559 | Cosine Similarity: 0.4221\n",
      "Epoch 7/15 | Loss: 0.2898 | Accuracy: 0.00%\n",
      "Test Loss: 0.3257 | Cosine Similarity: 0.4242\n",
      "Epoch 8/15 | Loss: 0.2612 | Accuracy: 0.00%\n",
      "Test Loss: 0.3166 | Cosine Similarity: 0.4251\n",
      "Epoch 9/15 | Loss: 0.2453 | Accuracy: 0.00%\n",
      "Test Loss: 0.2994 | Cosine Similarity: 0.4382\n",
      "Epoch 10/15 | Loss: 0.2335 | Accuracy: 0.00%\n",
      "Test Loss: 0.2902 | Cosine Similarity: 0.4585\n",
      "Epoch 11/15 | Loss: 0.2252 | Accuracy: 0.00%\n",
      "Test Loss: 0.2900 | Cosine Similarity: 0.5002\n",
      "Epoch 12/15 | Loss: 0.2199 | Accuracy: 0.00%\n",
      "Test Loss: 0.3222 | Cosine Similarity: 0.5085\n",
      "Epoch 13/15 | Loss: 0.2152 | Accuracy: 0.00%\n",
      "Test Loss: 0.3453 | Cosine Similarity: 0.5082\n",
      "Epoch 14/15 | Loss: 0.2117 | Accuracy: 0.00%\n",
      "Test Loss: 0.3443 | Cosine Similarity: 0.5066\n",
      "Epoch 15/15 | Loss: 0.2103 | Accuracy: 0.00%\n",
      "Test Loss: 0.3481 | Cosine Similarity: 0.5067\n"
     ]
    }
   ],
   "source": [
    "grad_classifier_model = GradientClassifier(input_dim=input_dim, output_dim=output_dim)\n",
    "grad_classifier_model.to(device)\n",
    "\n",
    "train_grad_classifier(grad_classifier_model, train_loader, test_loader, criterion_grad_classifier, num_epochs=num_epochs, lr=learning_rate, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "a04aafa8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.3481 | Cosine Similarity: 0.5067\n"
     ]
    }
   ],
   "source": [
    "test_preds = evaluate_model(grad_classifier_model, test_loader, criterion_grad_classifier, device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
