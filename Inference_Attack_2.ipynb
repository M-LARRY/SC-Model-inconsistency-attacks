{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cf6d5d8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader, TensorDataset, Subset, random_split\n",
    "from inference_attack import *\n",
    "from model import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a1bf8657",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "model = 1\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "rng_seed = 0\n",
    "dataset_name = \"MNIST\"                         # \"MNIST\" or \"CIFAR10\"\n",
    "data_path = \"./data\"\n",
    "\n",
    "# Part 1 Use pretrained model to create a dataset for gradient classification\n",
    "pretrained_model_path = \"trained_model_MNIST.pth\"\n",
    "criterion_pretrained_model = torch.nn.CrossEntropyLoss()\n",
    "num_classes = 10\n",
    "inference_dataset_size = 500\n",
    "batch_size_pretrained_model = 1\n",
    "\n",
    "# Part 2 Train gradient classifier on gradient dataset\n",
    "learning_rate_gradient_classifier = 0.001\n",
    "criterion_grad_classifier = kl_div\n",
    "# criterion_grad_classifier = nn.BCELoss()\n",
    "train_size = 0.7\n",
    "test_size = 1 - train_size\n",
    "num_epochs = 3\n",
    "batch_size_gradient_classifier = 16\n",
    "\n",
    "# Part 3 Make inference on estimated gradient update\n",
    "target_model_path = \"target_0.pth\"\n",
    "learning_rate_pretrained_model = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8fed30bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make reproducible\n",
    "random.seed(rng_seed)\n",
    "torch.manual_seed(rng_seed)\n",
    "np.random.seed(rng_seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e910d2fc",
   "metadata": {},
   "source": [
    "## Part 1. Use pretrained model to create a dataset for gradient classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a09708db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset (we use test datasets to train the model on samples that have not been used in training of the global model)\n",
    "if dataset_name == \"MNIST\":\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.1307,), (0.3081,))\n",
    "    ])\n",
    "    dataset = datasets.MNIST(root=data_path, train=False, download=True, transform=transform)\n",
    "    input_shape = (1, 28, 28)\n",
    "\n",
    "elif dataset_name == \"CIFAR10\":\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.4914, 0.4822, 0.4465),\n",
    "                             (0.2470, 0.2435, 0.2616))\n",
    "    ])\n",
    "    dataset = datasets.CIFAR10(root=data_path, train=False, download=True, transform=transform)\n",
    "    input_shape = (3, 32, 32)\n",
    "\n",
    "else:\n",
    "    raise ValueError(f\"Unsupported dataset: {dataset_name}\")\n",
    "\n",
    "indices = random.sample(range(len(dataset)), inference_dataset_size)\n",
    "dataset = Subset(dataset, indices=indices)\n",
    "\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size_pretrained_model, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "88bc0655",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load pretrained model\n",
    "theta_0 = torch.load(pretrained_model_path)\n",
    "\n",
    "if model == 1:\n",
    "    inference_model = Model1(input_shape, num_classes).to(device)\n",
    "elif model == 2:\n",
    "    inference_model = Model2(input_shape, num_classes).to(device)\n",
    "else:\n",
    "    print('Unknown model:', model)\n",
    "\n",
    "inference_model.load_state_dict(state_dict=theta_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b27d9069",
   "metadata": {},
   "outputs": [],
   "source": [
    "# collect gradient features\n",
    "features = []\n",
    "labels = []\n",
    "\n",
    "inference_model.to(device)\n",
    "inference_model.train() # TODO: eval can make some layers behave differently (Not in our case)\n",
    "\n",
    "for x, y in dataloader:\n",
    "    x, y = x.to(device), y.to(device)\n",
    "\n",
    "    # Forward pass\n",
    "    output = inference_model(x)\n",
    "    loss = criterion_pretrained_model(output, y)\n",
    "\n",
    "    # print(\"pr:\", output, \"gt:\", y)\n",
    "\n",
    "    # Compute gradients w.r.t. model parameters\n",
    "    grad = torch.autograd.grad(loss, inference_model.parameters(), retain_graph=False)\n",
    "    # grad = torch.autograd.grad(loss, [inference_model.fc1.weight, inference_model.fc1.bias], retain_graph=False)\n",
    "    grad_vector = torch.cat([g.view(-1) for g in grad])  # Flatten and concatenate\n",
    "\n",
    "    # Detach and store\n",
    "    features.append(grad_vector.detach().cpu().float())\n",
    "    # labels.append(one_hot_encode(y.item(), num_classes))\n",
    "    labels.append(multi_hot_encode(y, num_classes))\n",
    "    # print(multi_hot_encode(y, num_classes))\n",
    "    \n",
    "    # DEBUG\n",
    "    if torch.equal(multi_hot_encode(y, num_classes), multi_hot_encode(torch.tensor([7]), num_classes)):\n",
    "        saved_grad = grad_vector.unsqueeze(0)\n",
    "\n",
    "features = torch.stack(features)\n",
    "labels = torch.stack(labels).float()\n",
    "#labels = torch.stack([y.detach().clone().float32() for y in labels])\n",
    "\n",
    "dataset = TensorDataset(features, labels)\n",
    "input_dim = features.shape[1]\n",
    "output_dim = labels.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16963933",
   "metadata": {},
   "source": [
    "## Part 2. Train gradient classifier on gradient dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f3cba6f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "350 150\n"
     ]
    }
   ],
   "source": [
    "# dataset split\n",
    "tmp_train = int(train_size*inference_dataset_size/batch_size_pretrained_model)\n",
    "tmp_test = int(inference_dataset_size/batch_size_pretrained_model-tmp_train)\n",
    "print(tmp_train, tmp_test)\n",
    "\n",
    "train_dataset, test_dataset = random_split(dataset, [tmp_train, tmp_test])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size_gradient_classifier, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size_gradient_classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9834dc7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3 | Loss: 2.1371 | Accuracy: 0.00%\n",
      "Test Loss: 1.6636 | Cosine Similarity: 0.9656\n",
      "Epoch 2/3 | Loss: 1.5057 | Accuracy: 0.00%\n",
      "Test Loss: 1.4614 | Cosine Similarity: 1.0000\n",
      "Epoch 3/3 | Loss: 1.4612 | Accuracy: 0.00%\n",
      "Test Loss: 1.4612 | Cosine Similarity: 1.0000\n"
     ]
    }
   ],
   "source": [
    "# Model training\n",
    "grad_classifier_model = GradientClassifier(input_dim=input_dim, output_dim=output_dim)\n",
    "grad_classifier_model.to(device)\n",
    "\n",
    "train_grad_classifier(grad_classifier_model, train_loader, test_loader, criterion_grad_classifier, num_epochs=num_epochs, lr=learning_rate_gradient_classifier, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a04aafa8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 1.4612 | Cosine Similarity: 1.0000\n"
     ]
    }
   ],
   "source": [
    "# Model evaluation\n",
    "test_preds = evaluate_model(grad_classifier_model, test_loader, criterion_grad_classifier, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3449ad44",
   "metadata": {},
   "source": [
    "## Part 3. Make inference on estimated gradient update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "72fe697d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estimate the target gradient update as theta_0 - theta_1 / lr\n",
    "# theta_1 = torch.load(target_model_path)\n",
    "# est_grad = elementwise_diff_state_dicts(theta_0, theta_1, learning_rate_pretrained_model)\n",
    "# est_grad_vector = dict_to_tensor(est_grad).unsqueeze(0)\n",
    "# for key in est_grad.keys():\n",
    "#     print(est_grad[key].shape)\n",
    "\n",
    "# est_grad_vector = torch.cat([est_grad[key].view(-1) for key in est_grad.keys()]).unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7a112747",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "est_grad and true_grad 0.1514223642880097\n",
      "theta_0 and theta_0_D 1.0000001639127731\n",
      "theta_1 and theta_1_D 0.9999406039714813\n",
      "est_grad and est_grad_D 0.15142924978863448\n",
      "true_grad and est_grad_D 1.000000186264515\n",
      "----------\n",
      "1.000000186264515\n"
     ]
    }
   ],
   "source": [
    "# estimate update gradient and compare with true update\n",
    "theta_0 = torch.load('trained_model_MNIST.pth')\n",
    "theta_1 = torch.load('target_0.pth')\n",
    "est_grad = elementwise_diff_state_dicts(theta_0, theta_1, learning_rate_pretrained_model)\n",
    "\n",
    "theta_0_D = torch.load('DEBUG_initial_state.pth')\n",
    "theta_1_D = torch.load('DEBUG_updated_state.pth')\n",
    "est_grad_D = elementwise_diff_state_dicts(theta_0_D, theta_1_D, learning_rate_pretrained_model)\n",
    "\n",
    "true_grad = torch.load('DEBUG_true_grads.pth')\n",
    "\n",
    "print('est_grad and true_grad', state_dicts_average_cosine_similarity(est_grad, true_grad))\n",
    "print('theta_0 and theta_0_D', state_dicts_average_cosine_similarity(theta_0, theta_0_D))\n",
    "print('theta_1 and theta_1_D', state_dicts_average_cosine_similarity(theta_1, theta_1_D))\n",
    "print('est_grad and est_grad_D', state_dicts_average_cosine_similarity(est_grad, est_grad_D))\n",
    "print('true_grad and est_grad_D', state_dicts_average_cosine_similarity(true_grad, est_grad_D))\n",
    "print('----------')\n",
    "culo = elementwise_diff_state_dicts(theta_0, theta_1_D, learning_rate_pretrained_model)\n",
    "print(state_dicts_average_cosine_similarity(culo, true_grad))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ede59329",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0, 1.2203284986255641e-14, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Inference\n",
    "inferred = grad_classifier_model(est_grad_vector)\n",
    "# inferred = grad_classifier_model(saved_grad)\n",
    "inferred = inferred.tolist()[0]\n",
    "print(inferred)\n",
    "plt.bar(range(num_classes), inferred)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
