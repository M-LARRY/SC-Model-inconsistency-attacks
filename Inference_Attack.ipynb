{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cf6d5d8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import pandas as pd\n",
    "import random\n",
    "import json\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader, Subset, TensorDataset, random_split\n",
    "from inference_attack import *\n",
    "from model import Model2, GradientClassifier\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "691f1c3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# config\n",
    "dataset_filename = 'inference_dataset.csv'\n",
    "global_model = 'trained_model_MNIST.pth'\n",
    "target_update = 'global_update.pth'\n",
    "learning_rate = 0.01\n",
    "model = 2\n",
    "input_shape = (1, 28, 28)\n",
    "num_classes  = 10\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a09708db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset\n",
    "MNIST_dataset_loader = DataLoader(\n",
    "    dataset=MNIST_dataset,\n",
    "    batch_size=1,\n",
    "    shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c275594",
   "metadata": {},
   "outputs": [],
   "source": [
    "# collect gradient features\n",
    "features = []\n",
    "labels = []\n",
    "\n",
    "victim_model.to(device)\n",
    "victim_model.eval()  # Ensure model is in evaluation mode\n",
    "\n",
    "for x, y in MNIST_dataset_loader:\n",
    "    x, y = x.to(device), y.to(device)\n",
    "\n",
    "    # Forward pass\n",
    "    output = victim_model(x)\n",
    "    loss = criterion(output, y)\n",
    "\n",
    "    # Compute gradients w.r.t. model parameters\n",
    "    grads = torch.autograd.grad(loss, victim_model.parameters(), retain_graph=False)\n",
    "    grad_vector = torch.cat([g.view(-1) for g in grads])  # Flatten and concatenate\n",
    "\n",
    "    # Detach and store\n",
    "    features.append(grad_vector.detach().cpu().float())\n",
    "    labels.append(one_hot_encode(y.item(), num_classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "35de7740",
   "metadata": {},
   "outputs": [],
   "source": [
    "# estimate the gradient as theta_0 - theta_1 / lr\n",
    "theta_0 = torch.load(global_model)\n",
    "theta_1 = torch.load(target_update)\n",
    "\n",
    "est_grad = elementwise_diff_state_dicts(theta_0, theta_1, learning_rate)\n",
    "\n",
    "#print(est_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "22818d93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataset\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,)) # normalization params for MNIST\n",
    "])\n",
    "\n",
    "victim_model = Model2(input_shape, num_classes).to(device)\n",
    "victim_model.load_state_dict(state_dict=theta_0)\n",
    "\n",
    "MNIST_dataset = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
    "indices = random.sample(range(len(MNIST_dataset)), 100)\n",
    "MNIST_dataset = Subset(MNIST_dataset, indices=indices)\n",
    "MNIST_dataset_loader = DataLoader(dataset=MNIST_dataset, batch_size=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d317f052",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Model2(\n",
       "  (conv1): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (fc1): Linear(in_features=3136, out_features=256, bias=True)\n",
       "  (fc2): Linear(in_features=256, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "victim_model.train()\n",
    "victim_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "382ce35b",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Tensor' object has no attribute 'float16'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 21\u001b[39m\n\u001b[32m     19\u001b[39m grad = torch.autograd.grad(loss, victim_model.parameters())\n\u001b[32m     20\u001b[39m grad = torch.cat([g.view(-\u001b[32m1\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m g \u001b[38;5;129;01min\u001b[39;00m grad])\n\u001b[32m---> \u001b[39m\u001b[32m21\u001b[39m grad = \u001b[43mgrad\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdetach\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfloat16\u001b[49m()\n\u001b[32m     22\u001b[39m y = y[\u001b[32m0\u001b[39m].item()\n\u001b[32m     23\u001b[39m y = multihot_encode(y)\n",
      "\u001b[31mAttributeError\u001b[39m: 'Tensor' object has no attribute 'float16'"
     ]
    }
   ],
   "source": [
    "labels = []\n",
    "features = []\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "def multihot_encode(label):\n",
    "    out = []\n",
    "    for i in range(num_classes):\n",
    "        if i == label:\n",
    "            out.append(1)\n",
    "        else:\n",
    "            out.append(0)\n",
    "    return out\n",
    "\n",
    "for x, y in MNIST_dataset_loader:\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    output = victim_model(x)\n",
    "    loss = criterion(output, y)\n",
    "    grad = torch.autograd.grad(loss, victim_model.parameters())\n",
    "    grad = torch.cat([g.view(-1) for g in grad])\n",
    "    grad = grad.detach().cpu().float()\n",
    "    y = y[0].item()\n",
    "    y = multihot_encode(y)\n",
    "    labels.append(y)\n",
    "    features.append(grad)\n",
    "\n",
    "features_tensor = torch.stack(features)\n",
    "labels_tensor = torch.stack([torch.tensor(y, dtype=torch.float32) for y in labels])\n",
    "\n",
    "dataset = TensorDataset(features_tensor, labels_tensor)\n",
    "input_dim = features_tensor.shape[1]\n",
    "output_dim = labels_tensor.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c369e2c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.linalg import norm\n",
    "\n",
    "def train_grad_classifier(model, train_loader, test_loader, num_epochs=10, lr=1e-3, device='cuda'):\n",
    "    model = model.to(device)\n",
    "    criterion = nn.BCELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        all_preds = []\n",
    "        all_labels = []\n",
    "\n",
    "        for x_batch, y_batch in train_loader:\n",
    "            x_batch = x_batch.to(device)\n",
    "            y_batch = y_batch.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            outputs = model(x_batch)  # outputs are already sigmoid activated\n",
    "            loss = criterion(outputs, y_batch)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item() * x_batch.size(0)\n",
    "\n",
    "            preds = (outputs.detach().cpu().numpy() > 0.5).astype(int)\n",
    "            all_preds.append(preds)\n",
    "            all_labels.append(y_batch.cpu().numpy())\n",
    "\n",
    "        epoch_loss = running_loss / len(train_loader.dataset)\n",
    "\n",
    "        all_preds = np.vstack(all_preds)\n",
    "        all_labels = np.vstack(all_labels)\n",
    "        #f1 = f1_score(all_labels, all_preds, average='macro', zero_division=0)\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs} | Loss: {epoch_loss:.4f}\")\n",
    "        evaluate_model(model, test_loader, criterion, device)\n",
    "        \n",
    "\n",
    "def evaluate_model(model, test_loader, criterion, device='cuda'):\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "\n",
    "    total_loss = 0.0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    cosine_sims = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for x_batch, y_batch in test_loader:\n",
    "            x_batch = x_batch.to(device)\n",
    "            y_batch = y_batch.to(device)\n",
    "\n",
    "            outputs = model(x_batch)  # sigmoid output\n",
    "            loss = criterion(outputs, y_batch)\n",
    "            total_loss += loss.item() * x_batch.size(0)\n",
    "\n",
    "            preds = outputs.cpu().numpy()\n",
    "            labels = y_batch.cpu().numpy()\n",
    "\n",
    "            all_preds.append(preds)\n",
    "            all_labels.append(labels)\n",
    "\n",
    "            # Compute cosine similarity per sample\n",
    "            for p, l in zip(preds, labels):\n",
    "                # Avoid division by zero\n",
    "                if norm(p) == 0 or norm(l) == 0:\n",
    "                    cosine_sims.append(0.0)\n",
    "                else:\n",
    "                    cos_sim = np.dot(p, l) / (norm(p) * norm(l))\n",
    "                    cosine_sims.append(cos_sim)\n",
    "\n",
    "    avg_loss = total_loss / len(test_loader.dataset)\n",
    "    all_preds = np.vstack(all_preds)\n",
    "    all_labels = np.vstack(all_labels)\n",
    "    avg_cosine_similarity = np.mean(cosine_sims)\n",
    "\n",
    "    print(f\"Test Loss: {avg_loss:.4f} | Cosine Similarity: {avg_cosine_similarity:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f3cba6f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15 | Loss: 0.6897\n",
      "Test Loss: 0.6078 | Cosine Similarity: 0.3602\n",
      "Epoch 2/15 | Loss: 0.6274\n",
      "Test Loss: 0.6220 | Cosine Similarity: 0.3895\n",
      "Epoch 3/15 | Loss: 0.5654\n",
      "Test Loss: 0.7656 | Cosine Similarity: 0.3896\n",
      "Epoch 4/15 | Loss: 0.5211\n",
      "Test Loss: 1.0527 | Cosine Similarity: 0.3889\n",
      "Epoch 5/15 | Loss: 0.4828\n",
      "Test Loss: 1.0906 | Cosine Similarity: 0.3963\n",
      "Epoch 6/15 | Loss: 0.4496\n",
      "Test Loss: 1.0810 | Cosine Similarity: 0.4079\n",
      "Epoch 7/15 | Loss: 0.4166\n",
      "Test Loss: 0.9817 | Cosine Similarity: 0.4194\n",
      "Epoch 8/15 | Loss: 0.3904\n",
      "Test Loss: 0.9775 | Cosine Similarity: 0.4238\n",
      "Epoch 9/15 | Loss: 0.3641\n",
      "Test Loss: 0.9811 | Cosine Similarity: 0.4170\n",
      "Epoch 10/15 | Loss: 0.3395\n",
      "Test Loss: 0.9854 | Cosine Similarity: 0.4188\n",
      "Epoch 11/15 | Loss: 0.3434\n",
      "Test Loss: 0.9899 | Cosine Similarity: 0.4221\n",
      "Epoch 12/15 | Loss: 0.2922\n",
      "Test Loss: 0.9839 | Cosine Similarity: 0.4241\n",
      "Epoch 13/15 | Loss: 0.2754\n",
      "Test Loss: 0.9804 | Cosine Similarity: 0.4263\n",
      "Epoch 14/15 | Loss: 0.2568\n",
      "Test Loss: 1.0027 | Cosine Similarity: 0.4298\n",
      "Epoch 15/15 | Loss: 0.2419\n",
      "Test Loss: 1.0163 | Cosine Similarity: 0.4328\n"
     ]
    }
   ],
   "source": [
    "train_size = int(0.8 * len(dataset))\n",
    "test_size = len(dataset) - train_size\n",
    "\n",
    "train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32)\n",
    "\n",
    "grad_classifier_model = GradientClassifier(input_dim=features_tensor.shape[1], output_dim=labels_tensor.shape[1])\n",
    "grad_classifier_model.to(device)\n",
    "\n",
    "criterion = nn.BCELoss() \n",
    "optimizer = torch.optim.Adam(grad_classifier_model.parameters(), lr=1e-4)\n",
    "\n",
    "num_epochs = 15\n",
    "\n",
    "train_grad_classifier(grad_classifier_model, train_loader=train_loader, test_loader=test_loader, num_epochs=num_epochs, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a04aafa8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 1.0163 | Cosine Similarity: 0.4328\n"
     ]
    }
   ],
   "source": [
    "evaluate_model(grad_classifier_model, test_loader, criterion, device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
